import torch
import torch.nn as nn
from config import TransformerConfig
from model import PositionalEncoding, MultiHeadAttention, PositionWiseFFN, ResidualConnection, TransformerEncoder

def test_positional_encoding():
    """æµ‹è¯•ä½ç½®ç¼–ç ç»„ä»¶"""
    print("=" * 50)
    print("æµ‹è¯• PositionalEncoding...")
    
    # åˆ›å»ºé…ç½®å’Œæ¨¡å‹
    config = TransformerConfig()
    pe = PositionalEncoding(config)
    
    # åˆ›å»ºæµ‹è¯•è¾“å…¥ (batch_size=2, seq_len=10, d_model=128)
    batch_size, seq_len = 2, 10
    x = torch.zeros(batch_size, seq_len, config.d_model)
    
    print(f"è¾“å…¥å½¢çŠ¶: {x.shape}")
    
    # å‰å‘ä¼ æ’­
    output = pe(x)
    
    print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}")
    print(f"è¾“å…¥å’Œè¾“å‡ºæ˜¯å¦ç›¸åŒå½¢çŠ¶: {x.shape == output.shape}")
    print(f"ä½ç½®ç¼–ç èŒƒå›´: [{pe.pe.min():.4f}, {pe.pe.max():.4f}]")
    
    # æ£€æŸ¥ä¸åŒä½ç½®çš„ç¼–ç æ˜¯å¦ä¸åŒ
    pos1 = pe.pe[0, 0, :5]  # ç¬¬ä¸€ä¸ªä½ç½®çš„å‰5ä¸ªç»´åº¦
    pos2 = pe.pe[0, 1, :5]  # ç¬¬äºŒä¸ªä½ç½®çš„å‰5ä¸ªç»´åº¦
    print(f"ä½ç½®0çš„å‰5ä¸ªç»´åº¦: {pos1}")
    print(f"ä½ç½®1çš„å‰5ä¸ªç»´åº¦: {pos2}")
    print(f"ä¸åŒä½ç½®ç¼–ç æ˜¯å¦ä¸åŒ: {not torch.allclose(pos1, pos2)}")
    
    return True

def test_multihead_attention():
    """æµ‹è¯•å¤šå¤´æ³¨æ„åŠ›ç»„ä»¶"""
    print("\n" + "=" * 50)
    print("æµ‹è¯• MultiHeadAttention...")
    
    # åˆ›å»ºé…ç½®å’Œæ¨¡å‹
    config = TransformerConfig()
    mha = MultiHeadAttention(config)
    
    # åˆ›å»ºæµ‹è¯•è¾“å…¥ (batch_size=2, seq_len=8, d_model=128)
    batch_size, seq_len = 2, 8
    q = k = v = torch.randn(batch_size, seq_len, config.d_model)
    
    print(f"è¾“å…¥å½¢çŠ¶ - Q: {q.shape}, K: {k.shape}, V: {v.shape}")
    
    # å‰å‘ä¼ æ’­
    output, attn_weights = mha(q, k, v)
    
    print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}")
    print(f"æ³¨æ„åŠ›æƒé‡å½¢çŠ¶: {attn_weights.shape}")
    print(f"è¾“å‡ºå’Œè¾“å…¥Qæ˜¯å¦ç›¸åŒå½¢çŠ¶: {q.shape == output.shape}")
    
    # æ£€æŸ¥æ³¨æ„åŠ›æƒé‡çš„å±æ€§
    print(f"æ³¨æ„åŠ›æƒé‡èŒƒå›´: [{attn_weights.min():.4f}, {attn_weights.max():.4f}]")
    
    # æ›´ç²¾ç¡®åœ°æ£€æŸ¥softmaxå½’ä¸€åŒ–
    # æ³¨æ„åŠ›æƒé‡å½¢çŠ¶: (batch_size, n_heads, seq_len, seq_len)
    # å¯¹æœ€åä¸€ä¸ªç»´åº¦ï¼ˆseq_lenï¼‰æ±‚å’Œï¼Œæ¯ä¸ªä½ç½®å¯¹å…¶ä»–æ‰€æœ‰ä½ç½®çš„æ³¨æ„åŠ›æƒé‡åº”è¯¥å’Œä¸º1
    sums = attn_weights.sum(dim=-1)
    print(f"æ³¨æ„åŠ›æƒé‡æ¯è¡Œæ±‚å’Œçš„èŒƒå›´: [{sums.min():.6f}, {sums.max():.6f}]")
    
    # ç”±äºæµ®ç‚¹æ•°ç²¾åº¦ï¼Œä½¿ç”¨æ›´å®½æ¾çš„å®¹å·®
    is_normalized = torch.allclose(sums, torch.ones_like(sums), atol=1e-5)
    print(f"æ³¨æ„åŠ›æƒé‡æ˜¯å¦æ­£ç¡®å½’ä¸€åŒ– (å®¹å·®1e-5): {is_normalized}")
    
    # æ‰“å°å…·ä½“çš„æ•°å€¼ç¤ºä¾‹æ¥éªŒè¯
    print("\nå…·ä½“ç¤ºä¾‹ - ç¬¬ä¸€ä¸ªbatchï¼Œç¬¬ä¸€ä¸ªheadçš„æ³¨æ„åŠ›æƒé‡:")
    sample_weights = attn_weights[0, 0]
    for i in range(min(3, seq_len)):  # åªæ˜¾ç¤ºå‰3è¡Œ
        row = sample_weights[i]
        row_sum = row.sum().item()
        print(f"  ç¬¬{i}è¡Œ: sum={row_sum:.6f}, values={row[:3].tolist()}...")
    
    return is_normalized

def test_components_together():
    """æµ‹è¯•ä¸¤ä¸ªç»„ä»¶ä¸€èµ·å·¥ä½œ"""
    print("\n" + "=" * 50)
    print("æµ‹è¯•ç»„ä»¶ååŒå·¥ä½œ...")
    
    config = TransformerConfig()
    
    # åˆ›å»ºç»„ä»¶
    pe = PositionalEncoding(config)
    mha = MultiHeadAttention(config)
    
    # åˆ›å»ºæµ‹è¯•è¾“å…¥ (æ¨¡æ‹ŸåµŒå…¥åçš„è¾“å…¥)
    batch_size, seq_len = 2, 8
    x = torch.randn(batch_size, seq_len, config.d_model)
    
    print(f"åŸå§‹è¾“å…¥å½¢çŠ¶: {x.shape}")
    
    # å…ˆåº”ç”¨ä½ç½®ç¼–ç 
    x_with_pe = pe(x)
    print(f"æ·»åŠ ä½ç½®ç¼–ç åå½¢çŠ¶: {x_with_pe.shape}")
    
    # å†åº”ç”¨å¤šå¤´æ³¨æ„åŠ›
    output, attn_weights = mha(x_with_pe, x_with_pe, x_with_pe)
    print(f"å¤šå¤´æ³¨æ„åŠ›åå½¢çŠ¶: {output.shape}")
    
    # æ£€æŸ¥æ¢¯åº¦è®¡ç®—
    x.requires_grad_(True)
    x_with_pe = pe(x)
    output, _ = mha(x_with_pe, x_with_pe, x_with_pe)
    
    # è®¡ç®—æ¢¯åº¦
    loss = output.sum()
    loss.backward()
    
    print(f"æ¢¯åº¦è®¡ç®—æ­£å¸¸: {x.grad is not None}")
    print(f"æ¢¯åº¦å½¢çŠ¶: {x.grad.shape}")
    
    return True

def test_positionwise_ffn():
    """æµ‹è¯•å‰é¦ˆç½‘ç»œç»„ä»¶"""
    print("\n" + "=" * 50)
    print("æµ‹è¯• PositionWiseFFN...")
    
    config = TransformerConfig()
    ffn = PositionWiseFFN(config)
    
    # åˆ›å»ºæµ‹è¯•è¾“å…¥
    batch_size, seq_len = 2, 8
    x = torch.randn(batch_size, seq_len, config.d_model)
    
    print(f"è¾“å…¥å½¢çŠ¶: {x.shape}")
    
    # å‰å‘ä¼ æ’­
    output = ffn(x)
    
    print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}")
    print(f"è¾“å…¥è¾“å‡ºå½¢çŠ¶ä¸€è‡´: {x.shape == output.shape}")
    
    # æ£€æŸ¥æ¢¯åº¦
    x.requires_grad_(True)
    output = ffn(x)
    loss = output.sum()
    loss.backward()
    
    print(f"æ¢¯åº¦è®¡ç®—æ­£å¸¸: {x.grad is not None}")
    
    return True

def test_residual_connection():
    """æµ‹è¯•æ®‹å·®è¿æ¥"""
    print("\n" + "=" * 50)
    print("æµ‹è¯• ResidualConnection...")
    
    config = TransformerConfig()
    residual = ResidualConnection(config)
    
    # åˆ›å»ºä¸€ä¸ªç®€å•çš„å­å±‚ï¼ˆæ’ç­‰æ˜ å°„ï¼‰
    identity_sublayer = lambda x: x
    
    # åˆ›å»ºæµ‹è¯•è¾“å…¥
    batch_size, seq_len = 2, 8
    x = torch.randn(batch_size, seq_len, config.d_model)
    
    print(f"è¾“å…¥å½¢çŠ¶: {x.shape}")
    
    # å‰å‘ä¼ æ’­
    output = residual(x, identity_sublayer)
    
    print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}")
    print(f"æ®‹å·®è¿æ¥åå€¼ä¸åŒ: {not torch.allclose(x, output)}")
    
    return True

def test_transformer_encoder():
    """æµ‹è¯•å®Œæ•´Transformerç¼–ç å™¨"""
    print("\n" + "=" * 50)
    print("æµ‹è¯• TransformerEncoder...")
    
    config = TransformerConfig()
    encoder = TransformerEncoder(config)
    
    # åˆ›å»ºæµ‹è¯•è¾“å…¥ (token indices)
    batch_size, seq_len = 2, 16
    x = torch.randint(0, config.vocab_size, (batch_size, seq_len))
    
    print(f"è¾“å…¥å½¢çŠ¶: {x.shape}")
    print(f"è¯æ±‡è¡¨å¤§å°: {config.vocab_size}")
    
    # å‰å‘ä¼ æ’­
    output = encoder(x)
    
    print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}")
    print(f"ç¼–ç å™¨å±‚æ•°: {config.num_layers}")
    
    # æ£€æŸ¥å‚æ•°æ•°é‡
    total_params = sum(p.numel() for p in encoder.parameters())
    print(f"æ€»å‚æ•°æ•°é‡: {total_params:,}")
    
    return True

# åœ¨mainå‡½æ•°ä¸­æ·»åŠ æ–°æµ‹è¯•
if __name__ == "__main__":
    print("å¼€å§‹æµ‹è¯•Transformerç»„ä»¶...")
    
    try:
        test_positional_encoding()
        test_multihead_attention() 
        test_positionwise_ffn()
        test_residual_connection()
        test_transformer_encoder()
        test_components_together()
        
        print("\n" + "=" * 50)
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼ç»„ä»¶å®ç°æ­£ç¡®ã€‚")
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()